01 - Entender o problema:
    a) implementar uma classe abstrata para neurÃ´nios genÃ©ricos
    b) implementar uma classe derivada para neuronios ReLu
    c) implementar o mÃ©todo predict() (entrada + pesos + bias => saÃ­da)
    d) implementar uma main() para testar as classes derivadas

02 - Classe abstrata: classe que nÃ£o pode ser instanciada diretamente e serve
como base para outras classes. Ela define um modelo genÃ©rico que outras classes
derivadas devem seguir.
CaracterÃ­sticas:
    * ContÃ©m pelo menos um mÃ©todo virtual puro (= 0).
    * NÃ£o pode ser instanciada diretamente.
    * Ã‰ usada como base para heranÃ§a.
    * ForÃ§a a implementaÃ§Ã£o dos mÃ©todos na classe derivada.
Vantagens:
    * ForÃ§a a implementaÃ§Ã£o: Garante que todas as classes derivadas implementem
    os mÃ©todos necessÃ¡rios.
    * Permite polimorfismo: Objetos podem ser manipulados via ponteiros para a
    classe base.
    * CÃ³digo mais organizado: Define um modelo genÃ©rico para reutilizaÃ§Ã£o e expansÃ£o.
    * Facilita manutenÃ§Ã£o e escalabilidade: Se precisar adicionar um novo tipo de neurÃ´nio,
    o cÃ³digo antigo nÃ£o precisa ser alterado.

03 - Aprimorado o header da classe Neuronio:
a) passagem do parÃ¢metro entradas por referÃªncia constante (para evitar cÃ³pias)
b) uso de #ifndef e #define para proteÃ§Ã£o do cabeÃ§alho
c) estrutura clara para heranÃ§a

04 - Neuronio ReLU (Rectified Linear Unit): tipo de neurÃ´nio usado em redes neurais,
cuja principal caracterÃ­stica Ã© a funÃ§Ã£o de ativaÃ§Ã£o ReLu.
Realiza trÃªs operaÃ§Ãµes principais:
    a) MultiplicaÃ§Ã£o das entradas pelos pesos
    b) Soma ponderada das entradas, junto com o bias
    c) A funÃ§Ã£o de ativaÃ§Ã£o decide se o neurÃ´nio dispara ou nÃ£o
AtivaÃ§Ã£o:
    x < 0 -> 0
    x >= 0 -> x
Vantagens:
    a) Evita o Vanishing Gradient (de outros modelos)
    b) Computacionalmente eficiente
    c) Facilita o aprendizado de redes profundas

05 - Implementar a classe NeuronioReLu
O que precisa fazer?
    * Herdar de Neuronio (a classe base abstrata).
    * Implementar o mÃ©todo predict(), que calcula a saÃ­da do neurÃ´nio usando a funÃ§Ã£o ReLU.
    * Utilizar a fÃ³rmula correta para a saÃ­da:
        saida = max(0, âˆ‘(ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘ğ‘‘ğ‘ğ‘– Ã— ğ‘ğ‘’ğ‘ ğ‘œğ‘–) + bias)
    * Garantir que a classe estÃ¡ bem encapsulada e eficiente.

06 - Classe NeuronioReLu
O que foi feito?
    * Herdado Neuronio e implementado predict()
    * Usado loop for para calcular o somatÃ³rio ponderado
    * Aplicada a funÃ§Ã£o max para seguir a regra da ReLu
    * Usado override para garantir a subscriÃ§Ã£o do mÃ©todo predict()
    * Usado size_t como tipo seguro para Ã­ndices de vetor (evita problemas com nÃºmeros negativos)

07 - Implementar a main()
Cumprir os requesitos:
    * Utilizar um mÃ©todo abstrato para implementar uma classe concreta.
    * Trabalhe com polimorfismo para calcular a saÃ­da.
    * Manipule objetos das classes derivadas atravÃ©s de ponteiros para a classe base.
    * Use vetores ou arrays para representar entradas e pesos.
    * Implementar a main com mais variaÃ§Ãµes de casos do que os apresentados nos exemplos.

08 - NÃ£o foi possÃ­vel acessar pesos na main porque foi declarada como protected na classe Neuronio,
Isso acontece porque protected permite acesso apenas dentro da prÃ³pria classe e de classes derivadas.
SoluÃ§Ã£o: adicionei um mÃ©todo pÃºblico getPesos() na classe Neuronio

09 - ConclusÃ£o:
    * O cÃ³digo atende a todos os requisitos
    * Testados 10 neurÃ´nios com 10 vetores de entrada variados (2 a 5 elementos)
    * IncluÃ­dos os exemplos do enunciado
    * O gerenciamento de memÃ³ria estÃ¡ bem estruturado


