01 - Entender o problema:
    a) implementar uma classe abstrata para neurônios genéricos
    b) implementar uma classe derivada para neuronios ReLu
    c) implementar o método predict() (entrada + pesos + bias => saída)
    d) implementar uma main() para testar as classes derivadas

02 - Classe abstrata: classe que não pode ser instanciada diretamente e serve
como base para outras classes. Ela define um modelo genérico que outras classes
derivadas devem seguir.
Características:
    * Contém pelo menos um método virtual puro (= 0).
    * Não pode ser instanciada diretamente.
    * É usada como base para herança.
    * Força a implementação dos métodos na classe derivada.
Vantagens:
    * Força a implementação: Garante que todas as classes derivadas implementem
    os métodos necessários.
    * Permite polimorfismo: Objetos podem ser manipulados via ponteiros para a
    classe base.
    * Código mais organizado: Define um modelo genérico para reutilização e expansão.
    * Facilita manutenção e escalabilidade: Se precisar adicionar um novo tipo de neurônio,
    o código antigo não precisa ser alterado.

03 - Aprimorado o header da classe Neuronio:
a) passagem do parâmetro entradas por referência constante (para evitar cópias)
b) uso de #ifndef e #define para proteção do cabeçalho
c) estrutura clara para herança

04 - Neuronio ReLU (Rectified Linear Unit): tipo de neurônio usado em redes neurais,
cuja principal característica é a função de ativação ReLu.
Realiza três operações principais:
    a) Multiplicação das entradas pelos pesos
    b) Soma ponderada das entradas, junto com o bias
    c) A função de ativação decide se o neurônio dispara ou não
Ativação:
    x < 0 -> 0
    x >= 0 -> x
Vantagens:
    a) Evita o Vanishing Gradient (de outros modelos)
    b) Computacionalmente eficiente
    c) Facilita o aprendizado de redes profundas

05 - Implementar a classe NeuronioReLu
O que precisa fazer?
    * Herdar de Neuronio (a classe base abstrata).
    * Implementar o método predict(), que calcula a saída do neurônio usando a função ReLU.
    * Utilizar a fórmula correta para a saída:
        saida = max(0, ∑(𝑒𝑛𝑡𝑟𝑎𝑑𝑎𝑖 × 𝑝𝑒𝑠𝑜𝑖) + bias)
    * Garantir que a classe está bem encapsulada e eficiente.

06 - Classe NeuronioReLu
O que foi feito?
    * Herdado Neuronio e implementado predict()
    * Usado loop for para calcular o somatório ponderado
    * Aplicada a função max para seguir a regra da ReLu
    * Usado override para garantir a subscrição do método predict()
    * Usado size_t como tipo seguro para índices de vetor (evita problemas com números negativos)

07 - 